{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann Machines: A Comprehensive Introduction\n",
    "\n",
    "## Overview\n",
    "A **Boltzmann Machine (BM)** is a stochastic recurrent neural network inspired by statistical mechanics. Developed by Geoffrey Hinton and Terrence Sejnowski (1985), it serves as a **generative model** for learning complex probability distributions.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Architecture\n",
    "- **Neurons**: Binary units (typically {0,1} or {-1,1})\n",
    "- **Connections**: \n",
    "  - Symmetric weights (wᵢⱼ = wⱼᵢ)\n",
    "  - No self-connections\n",
    "- **Layers**:\n",
    "  - **Visible units**: Represent observed data\n",
    "  - **Hidden units**: Capture latent features\n",
    "\n",
    "### 2. Energy Function\n",
    "The energy of state (v,h) is defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann Machines\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### Energy Function\n",
    "The energy of a joint configuration $(v, h)$ is given by:\n",
    "\n",
    "$$\n",
    "E(v, h) = -\\sum_i a_i v_i - \\sum_j b_j h_j - \\sum_{i,j} v_i w_{i,j} h_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_i$: visible unit $i$\n",
    "- $h_j$: hidden unit $j$ \n",
    "- $a_i$: bias for visible unit $i$\n",
    "- $b_j$: bias for hidden unit $j$\n",
    "- $w_{i,j}$: weight between units $i$ and $j$\n",
    "\n",
    "### Probability Distribution\n",
    "The probability of a configuration follows the Boltzmann distribution:\n",
    "\n",
    "$$\n",
    "P(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n",
    "$$\n",
    "\n",
    "Where $Z$ is the partition function:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{v,h} e^{-E(v, h)}\n",
    "$$\n",
    "\n",
    "### Conditional Probabilities\n",
    "For an RBM, the conditional probabilities factorize:\n",
    "\n",
    "$$\n",
    "P(h|v) = \\prod_j P(h_j|v) \\\\\n",
    "P(v|h) = \\prod_i P(v_i|h)\n",
    "$$\n",
    "\n",
    "With individual unit activations:\n",
    "\n",
    "$$\n",
    "P(h_j=1|v) = \\sigma(b_j + \\sum_i v_i w_{i,j}) \\\\\n",
    "P(v_i=1|h) = \\sigma(a_i + \\sum_j h_j w_{i,j})\n",
    "$$\n",
    "\n",
    "Where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the sigmoid function.\n",
    "\n",
    "## Training Algorithm (Contrastive Divergence)\n",
    "\n",
    "1. **Positive Phase**:\n",
    "   Compute $\\langle v_i h_j \\rangle_{data}$ using $P(h|v^{(0)})$\n",
    "\n",
    "2. **Negative Phase**:\n",
    "   - Sample $h^{(0)} \\sim P(h|v^{(0)})$\n",
    "   - Sample $v^{(1)} \\sim P(v|h^{(0)})$\n",
    "   - Sample $h^{(1)} \\sim P(h|v^{(1)})$\n",
    "   Compute $\\langle v_i h_j \\rangle_{recon}$\n",
    "\n",
    "3. **Weight Update**:\n",
    "   $$\n",
    "   \\Delta w_{i,j} = \\eta (\\langle v_i h_j \\rangle_{data} - \\langle v_i h_j \\rangle_{recon})\n",
    "   $$\n",
    "\n",
    "## Implementation Pseudocode\n",
    "\n",
    "```python\n",
    "def train_rbm(data, n_visible, n_hidden, learning_rate, epochs):\n",
    "    # Initialize parameters\n",
    "    W = random_normal([n_visible, n_hidden])\n",
    "    a = zeros([n_visible])  # visible biases\n",
    "    b = zeros([n_hidden])   # hidden biases\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Positive phase\n",
    "        h0_prob = sigmoid(data @ W + b)\n",
    "        h0_sample = binomial(h0_prob)\n",
    "        \n",
    "        # Negative phase\n",
    "        v1_prob = sigmoid(h0_sample @ W.T + a)\n",
    "        v1_sample = binomial(v1_prob)\n",
    "        h1_prob = sigmoid(v1_sample @ W + b)\n",
    "        \n",
    "        # Update parameters\n",
    "        grad_W = data.T @ h0_prob - v1_sample.T @ h1_prob\n",
    "        grad_a = mean(data - v1_sample, axis=0)\n",
    "        grad_b = mean(h0_prob - h1_prob, axis=0)\n",
    "        \n",
    "        W += learning_rate * grad_W\n",
    "        a += learning_rate * grad_a\n",
    "        b += learning_rate * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.1, epochs=100):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.a = np.zeros(n_visible)  # Visible layer bias\n",
    "        self.b = np.zeros(n_hidden)   # Hidden layer bias\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sample_hidden(self, v):\n",
    "        # Sample hidden layer given visible layer\n",
    "        h_prob = self.sigmoid(np.dot(v, self.W) + self.b)\n",
    "        h_sample = np.random.binomial(1, h_prob)\n",
    "        return h_prob, h_sample\n",
    "    \n",
    "    def sample_visible(self, h):\n",
    "        # Sample visible layer given hidden layer\n",
    "        v_prob = self.sigmoid(np.dot(h, self.W.T) + self.a)\n",
    "        v_sample = np.random.binomial(1, v_prob)\n",
    "        return v_prob, v_sample\n",
    "    \n",
    "    def train(self, X):\n",
    "        errors = []\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass: compute hidden layer probabilities and sample\n",
    "            h0_prob, h0_sample = self.sample_hidden(X)\n",
    "            \n",
    "            # Backward pass: reconstruct visible layer and sample hidden layer\n",
    "            v1_prob, v1_sample = self.sample_visible(h0_sample)\n",
    "            h1_prob, h1_sample = self.sample_hidden(v1_sample)\n",
    "            \n",
    "            # Update weights and biases using Contrastive Divergence (CD-1)\n",
    "            positive_grad = np.dot(X.T, h0_prob)\n",
    "            negative_grad = np.dot(v1_sample.T, h1_prob)\n",
    "            \n",
    "            self.W += self.lr * (positive_grad - negative_grad) / X.shape[0]\n",
    "            self.a += self.lr * np.mean(X - v1_sample, axis=0)\n",
    "            self.b += self.lr * np.mean(h0_prob - h1_prob, axis=0)\n",
    "            \n",
    "            # Calculate reconstruction error\n",
    "            error = np.mean((X - v1_prob) ** 2)\n",
    "            errors.append(error)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Error: {error:.4f}\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        _, h = self.sample_hidden(X)\n",
    "        _, v = self.sample_visible(h)\n",
    "        return v\n",
    "\n",
    "# Load MNIST dataset (binarized)\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype('float32') / 255.0\n",
    "X = Binarizer(threshold=0.5).fit_transform(X)  # Binarize\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RBM\n",
    "rbm = RBM(n_visible=784, n_hidden=64, learning_rate=0.01, epochs=50)\n",
    "errors = rbm.train(X_train)\n",
    "\n",
    "# Visualize training error\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.title(\"RBM Training Error\")\n",
    "plt.show()\n",
    "\n",
    "# Test reconstruction\n",
    "test_sample = X_test[:5]\n",
    "reconstructed = rbm.reconstruct(test_sample)\n",
    "\n",
    "# Visualize original vs. reconstructed images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(test_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(\"Original\")\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title(\"Reconstructed\")\n",
    "    axes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
